{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harald-gen01/generatia_play/blob/main/generate_propmts_v0_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This colab generates two prompts, rank them and give you the best option."
      ],
      "metadata": {
        "id": "xM33MKkfjNQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain > /dev/null 2>&1\n",
        "!pip install langchain-openai > /dev/null 2>&1\n",
        "!pip install langchainhub > /dev/null 2>&1"
      ],
      "metadata": {
        "id": "hdwc2mNHrpoj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.agents import Tool\n",
        "from langchain.llms.openai import OpenAI\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.language_models.llms import LLM"
      ],
      "metadata": {
        "id": "XtPrswxizYHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "# Retrieve and set the OpenAI API key\n",
        "openai_api_key = userdata.get('openai_api_key')\n",
        "\n",
        "# Initialize the ChatOpenAI object with the OpenAI API key\n",
        "llm = ChatOpenAI(openai_api_key=openai_api_key,model=\"gpt-3.5-turbo\", temperature=.8)"
      ],
      "metadata": {
        "id": "UxklNAsjzpKM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import tool\n",
        "\n",
        "@tool\n",
        "def generate_prompt(task_description: str) -> str:\n",
        "    \"\"\"\n",
        "    Generate a prompt based on the task description using GPT-3.5.\n",
        "    \"\"\"\n",
        "    messages = [\n",
        "    (\"system\", \"You are an experienced chatgp4 llm prompt engineer that generate awesome llm prompts\"),\n",
        "\n",
        "    (\"human\", f\"Generate a detailed, engaging prompt for the task: {task_description}\"),\n",
        "    ]\n",
        "\n",
        "    return llm.invoke(messages)\n",
        "\n",
        "# Generate one or more prompts\n",
        "\n",
        "results = []\n",
        "for i in range(2):\n",
        "    result = generate_prompt(\"Describe the impact of virtual reality on video survelliance\")  # Correct function call\n",
        "    results.append(result)\n",
        "\n",
        "output1, output2 = results\n",
        "\n",
        "# print(output1)\n",
        "# print(output2)"
      ],
      "metadata": {
        "id": "NJZpPFvJdr2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import tool\n",
        "\n",
        "@tool\n",
        "def rank_outputs(task_description: str, output1: str, output2: str):\n",
        "    \"\"\"\n",
        "    Compare two generated prompts and directly print a comparison prompt.\n",
        "\n",
        "    Parameters:\n",
        "        task_description (str): Description of the task for which the prompts were generated.\n",
        "        output1 (str): Text of the first generated prompt.\n",
        "        output2 (str): Text of the second generated prompt.\n",
        "    \"\"\"\n",
        "    # Create a comparison prompt directly using the outputs\n",
        "    comparison_prompt = (f\"Compare these two responses based on the task: {task_description}\\n\"\n",
        "                         f\"Response A: {output1}\\n\"\n",
        "                         f\"Response B: {output2}\\n\"\n",
        "                         \"Which response is better, A or B?\")\n",
        "\n",
        "    ranking_result = llm.invoke(comparison_prompt, max_tokens=50)\n",
        "\n",
        "    results = ranking_result\n",
        "\n",
        "    return \"A\" if \"A is better\" in ranking_result else \"B\"\n",
        "\n",
        "# print(results)"
      ],
      "metadata": {
        "id": "bA_nPEYiRt16"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tools = [generate_prompt, rank_outputs]\n",
        "\n",
        "# print(tools)"
      ],
      "metadata": {
        "id": "Ja8we9L04-Y4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create Agent"
      ],
      "metadata": {
        "id": "27PXS_VF5HRw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(openai_api_key=openai_api_key,model=\"gpt-4\", temperature=.8)"
      ],
      "metadata": {
        "id": "fgVtY64R5JIc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import hub\n",
        "\n",
        "# Get the prompt to use - you can modify this!\n",
        "\n",
        "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
        "prompt.messages"
      ],
      "metadata": {
        "id": "k23ZNw7C0Owe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import create_tool_calling_agent\n",
        "\n",
        "agent = create_tool_calling_agent(llm, tools, prompt)"
      ],
      "metadata": {
        "id": "Ye9YLci25Wd6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import AgentExecutor\n",
        "\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
      ],
      "metadata": {
        "id": "pRXqx5sa0hSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check agent executor, optional\n",
        "\n",
        "agent_executor.invoke({\"input\": \"hi!\"})"
      ],
      "metadata": {
        "id": "ZN50X7Uo0lew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Your prompt requirement\n",
        "\n",
        "agent_executor.invoke({\"input\": \"give me the best prompt to create a 3 tier GCP architecture\"})"
      ],
      "metadata": {
        "id": "_P-__wvq0pkq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}